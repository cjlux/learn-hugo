---
title: "D√©tection d'objets avec tensorflow2"
menu:
  main:
    name: "D√©tection d'objets avec tf2"
    weight: 3
    parent: "vision"
---

Dans cette section nous proposons d'utiliser l'API __Tensorflow Object Detection__ (_a.k.a_ TOD) qui propose :
* une collection de r√©seaux d√©j√† entra√Æn√©s sp√©cialement con√ßus pour pour la d√©tection d'objets dans des images (__Object Detection__),
* le m√©canisme de _transfert learning_ pour continuer l'entra√Ænement des r√©seaux pr√©-entra√Æn√©s avec nos propres images labellis√©es, 
pour obtenir la d√©tection des objets qui nous int√©ressent.

Contrairement √† la strat√©gie de __Classification__ pr√©sent√©e dans dans la section [Classification tf2](https://learn.e.ros4.pro/fr/vision/classification_tf2/), 
la __D√©tection d'objets__ permet de trouver directement les bo√Ætes englobantes des objets "face de cube avec un 1" et "face de cube avec un 2".

Cette approche √©vite la phase de traitement d'image classique pour extraire puis classifier les images des faces des cubes. 

Le traitement d'image bas√© sur une approche traditionnelle de manipulation des pixels de l'image (seuillage, extraction de contour, segmentation...) reste assez fragile : en particulier il est  sensible √† la luminosit√©, √† la pr√©sence ou non d'un fond noir... Un avantage attendu de l'approcje Object Detection est de fournir directement les bo√Ætes englobantes des faces des cubes sans passer par une √©tapde de traitement d'image.

## Pr√©requis

* Bonne compr√©hension de Python et numpy
* Une premi√®re exp√©rience des r√©seaux de neurones est souhaitable.

L'entra√Ænement des r√©seaux de neurones avec le module `tensorflow` se fera de pr√©f√©rence dans un environnement virtuel Python (EVP) qui permet de travailler dans un environnement Python d√©di√©.
Vous pouvez vous r√©ferrer √† la [FAQ Python : environnement virtuel](https://learn.e.ros4.pro/fr/faq/python_venv/) 

## 1. Documentation

Documentation g√©n√©rale sur numpy :
* [Numpy cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf)
* [NumPy quickstart](https://numpy.org/devdocs/user/quickstart.html)

Documentation sur l'_API TOD_ pour `tensorflow2` :
* Le tutoriel officiel complet : [TensorFlow 2 Object Detection API tutorial](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/index.html)
* Le d√©p√¥t git : [models/research/object_detection](https://github.com/tensorflow/models/tree/master/research/object_detection)

Ce tutoriel peut √™tre consult√© pour aller chercher des d√©tails qui ne sont pas d√©velopp√©s dans l'activit√© propos√©e, mais il est pr√©ferrable de suivre 
les indications du paragraphe suivant pour installer une version r√©cente de tensorflow2. 

## 2. Installation de l'API TOD

### 2.1 Clonage du d√©p√¥t `tensorflow/models`

Cr√©√© un dossier sp√©cifique pour le travail avec l'API TOD en clonant le d√©p√¥t github `cjlux/tod_tf2.git` :

```bash
(tf2) jlc@pikatchou~$ cd <quelque_part>   # choisis le r√©pertoire o√π cloner `tod_tf2.git`, par exemple : ~/catkin_ws/
(tf2) jlc@pikatchou~$ git clone https://github.com/cjlux/tod_tf2.git
```
Le clonage cr√©√© le dossier `tod_tf2` contenant des scripts Python qui seront utilis√©s plus tard. Ce dossier est la racine du projet.<br>
Dans le dossier `tod_tf2` clone le d√©p√¥t github `tensorflow/models`¬†:
```bash
(tf2) jlc@pikatchou~$ cd tod_tf2
(tf2) jlc@pikatchou~$ git clone https://github.com/tensorflow/models.git
```

Tu obtiens un dossier `models` : l‚ÄôAPI TOD est dans le dossier `models/research/object_detection` :
```bash	
(tf2) jlc@pikatchou~$ tree -d -L 2 .
.
‚îî‚îÄ‚îÄ models
    ‚îú‚îÄ‚îÄ community
    ‚îú‚îÄ‚îÄ official
    ‚îú‚îÄ‚îÄ orbit
    ‚îî‚îÄ‚îÄ research
```	

Compl√®te ton installation avec quelques paquets Python utiles pour le travail avec l'API TOD :
```bash
(tf2) jlc@pikatchou $ conda install cython contextlib2 pillow lxml
(tf2) jlc@pikatchou $ pip install labelimg rospkg
```
Mets √† jour la variable d‚Äôenvironnement `PYTHONPATH` en ajoutant √† la fin du fichier ~/.bashrc les deux lignes¬†:
```bash
export TOD_TF2="<chemin absolu du dossier tod_tf2>"
export PYTHONPATH=$TOD_TF2/models:$TOD_TF2/models/research:$PYTHONPATH
```
remplace `"<chemin absolu du dossier tod_tf2>"` par le chemin absolu du dossier `tod_tf2` sur ta machine.

Lance un nouveau terminal pour activer le nouvel environnement shell¬†; tout ce qui suit sera fait dans ce nouveau terminal.

### 2.2 Installer les outils `protobuf`

L‚ÄôAPI native TOD utilise des fichiers `*.proto`¬†pour la configuration des mod√®les et le stockage des param√®tres d‚Äôentra√Ænement. 
Ces fichiers doivent √™tre traduits en fichiers `*.py` afin que l‚ÄôAPI Python puisse fonctionner correctement.  

Du dois installer en premier la commande `protoc`¬†:
```bash
(tf2) jlc@pikatchou $ sudo apt install protobuf-compiler
```
Tu peux ensuite te positionner dans le dossier `tod_tf2/models/research` et taper¬†:
```bash
# From tod_tf2/models/research/
(tf2) jlc@pikatchou $ protoc object_detection/protos/*.proto  --python_out=.
```
Cette commande travaille de fa√ßon muette.

### 2.3 Installer l'API COCO

COCO est une banque de donn√©es destin√©e √† alimenter les algorithmes de d√©tection d‚Äôobjets, de segmentation‚Ä¶ voir [cocodataset.org](https://cocodataset.org) pour les tutoriels, publications‚Ä¶ 

üíª Pour installer l‚ÄôAPI Python de COCO, clone le site cocoapi dans le dossie `/tmp`, tape la commande `make` dans le dossier `cocoapi/PythonAPI`, puis recopie le dossier `pycococtools` dans `models/research/`¬†:
```bash
(tf2) jlc@pikatchou~$ cd /tmp
(tf2) jlc@pikatchou~$ git clone  https://github.com/cocodataset/cocoapi.git
(tf2) jlc@pikatchou~$ cd cocoapi/PythonAPI/
(tf2) jlc@pikatchou~$ make
(tf2) jlc@pikatchou~$ cp -r pycocotools/ <chemin absolu du dossier tod_tf2>/models/research/
```
### 2.4 Finalisation 

üíª Pour finir l'installation, place-toi dans le dossier  `models/research/` et tape les commandes :
```bash
# From tod_tf2/models/research/
(tf2) jlc@pikatchou $ cp object_detection/packages/tf2/setup.py .
(tf2) jlc@pikatchou $ python setup.py build
(tf2) jlc@pikatchou $ pip install .
```

### 2.5 Tester l'installation de l'API TOD

üíª Pour tester ton installation de l‚ÄôAPI TOD, place-toi dans le dossier `models/research/` et tape la commande¬†:
```bash	
# From within tod_tf2/models/research/
(tf2) jlc@pikatchou~$ python object_detection/builders/model_builder_tf2_test.py
```
Le programme d√©roule toute une s√©rie de tests et doit se terminer par un OK¬†:

	...
	[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size
	[ RUN      ] ModelBuilderTF2Test.test_session
	[  SKIPPED ] ModelBuilderTF2Test.test_session
	[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor
	INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s
	I0505 18:19:38.639148 140634691176256 test_util.py:2075] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s
	[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor
	[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture
	INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s
	I0505 18:19:38.640017 140634691176256 test_util.py:2075] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s
	[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture
	[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor
	INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s
	I0505 18:19:38.641987 140634691176256 test_util.py:2075] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s
	[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor
	----------------------------------------------------------------------
	Ran 21 tests in 53.105s

	OK (skipped=1)

üíª Pour finir, tu peux v√©rifier l‚Äôinstallation en utilisant un cahier IPython de l'API TOD ¬†: dans le dossier `models/research/object_detection` lance la commande jupyter notebook et charge le cahier `colab/object_detection_tutorial.ipynb` : 

* ‚ö†Ô∏è Avant d'ex√©cuter les cellules du notebook, il faut corriger une erreur dans le fichier `.../miniconda3/envs/tf2/lib/python3.8/site-packages/object_detection/utils/ops.py`, ligne 825 :
remplacer `tf.uint8` par `tf.uint8.as_numpy_dtype`
* ‚ö†Ô∏è Attention √† ne pas exc√©cuter les cellules du notebook comportant les commandes `!pip install ...` ou qui contiennent la _magic chain_ `%%bash`¬†:	

![notebook_test_TOD.png](img/notebook_test_TOD.png)

Ex√©cute les cellules une √† une, tu ne dois pas avoir d‚Äôerreur :

* La partie "__Detection__" (qui dure quelques secondes ou quelques minutes suivant ton CPU‚Ä¶) utilise le r√©seau pr√©-entra√Æn√© `ssd_mobilenet_v1_coco_2017_11_17` pour d√©tecter des objets dans les   images¬†de test :	

![notebook_test_TOD_image1et2.png](img/notebook_test_TOD_image1et2.png)

* La partie "__Instance Segmentation__" utilise le r√©seau pr√©-entra√Æn√© `mask_rcnn_inception_resnet_v2_atrous_coco_2018_01_28` pour d√©tecter les objets et leurs masques, par exemple :

![notebook_test_TOD_image-mask1.png](img/notebook_test_TOD_image-mask1.png)

La suite du travail se d√©compose ainsi :
* Cr√©er l'arborescence de travail
* T√©l√©charger le r√©seau pr√©-entra√Æn√©
* Cr√©er la banque d'images labellis√©es pour l'entra√Ænement supervis√© du r√©seau choisi
* Entra√Æner le r√©seau avec la banque d'images labellis√©es.
* √âvaluer les inf√©rences du r√©seau avec les images de test
* Int√©grer de l'exploitation du r√©seau dans l'environnement ROS.

## 3. Cr√©er l'arborescence de travail

L'arborescence g√©n√©rique de travail propos√©e pour cette activit√© est la suivante :

	tod_tf2
	‚îú‚îÄ‚îÄ images
	‚îÇ   ‚îî‚îÄ‚îÄ<project>
	‚îÇ       ‚îú‚îÄ‚îÄ test
	‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ *.jpg, *.png ... *.xml
	‚îÇ       ‚îú‚îÄ‚îÄ train
	‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ *.jpg, *.png ... *.xml
	‚îÇ       ‚îî‚îÄ‚îÄ *.csv
	‚îú‚îÄ‚îÄ pre_trained
	‚îÇ	‚îî‚îÄ‚îÄ <pre_trained-network>
	‚îú‚îÄ‚îÄ training
	‚îÇ   ‚îú‚îÄ‚îÄ<project>
	‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ <pre_trained-network>
	‚îÇ   ‚îú‚îÄ‚îÄ train.record
	‚îÇ   ‚îú‚îÄ‚îÄ test.record
	‚îÇ   ‚îî‚îÄ‚îÄ label_map.txt
	‚îî‚îÄ‚îÄ models
	    ‚îî‚îÄ‚îÄ research
	        ‚îî‚îÄ‚îÄ object_detection
	
	

* Le dossier `images/` contient un dossier pour chaque projet, avec dedans :
	* les dossiers `test` et `train` qui contiennent chacun :
		* les images (\*.png, \*.jpg) √† analyser,
		* les fichiers d'annotation (\*.xml) faits avec le logiciel `labelImg` : ils donnent, pour chacun des objets d'une image, les coordonn√©es de la bo√Æte englobante et le label de l'objet.
	* les fichiers d'annotation \*.cvs (convertis au format CSV), qui seront √† leur tour convertis au format _tensorflow record_.
* Le dossier `pre_trained/` contient un sous-dossier pour chacun des r√©seaux pr√©-entrain√©s utilis√©.
* le dossier `training/` contient  √©galement un dossier pour chaque projet, avec dedans :
	* un dossier pour chacun des r√©seaux pr√©-entrain√©s utilis√© : c'est dans ce dossier que seront stock√©s les fichiers des poids du r√©seau entra√Æn√©,
	* les fichiers `train.reccord`  et `test.reccord` : contiennent les donn√©es labelis√©es d'entra√Ænement et de test converties du format CSV au format _tensorflow record_,
	* le fichier `label_map.txt` : liste les labels correpondants aux objets √† d√©tecter.
	
Pour la d√©tection des faces des cubes dans les images faites par le robot Poppy Ergo Jr, le dossier `<project>` sera nomm√© `faces_cubes`, ce qui donne l'arborescence de travail :

	tod_tf2
	‚îú‚îÄ‚îÄ images
	‚îÇ   ‚îî‚îÄ‚îÄ faces_cubes
	‚îÇ       ‚îú‚îÄ‚îÄ test
	‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ *.jpg, *.png ... *.xml
	‚îÇ       ‚îú‚îÄ‚îÄ train
	‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ *.jpg, *.png ... *.xml
	‚îÇ       ‚îî‚îÄ‚îÄ *.csv
	‚îú‚îÄ‚îÄ pre_trained
	‚îÇ	‚îî‚îÄ‚îÄ <pre_trained-network>
	‚îú‚îÄ‚îÄ training
	‚îÇ   ‚îú‚îÄ‚îÄ faces_cubes
	‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ <pre_trained-network>
	‚îÇ   ‚îú‚îÄ‚îÄ train.record
	‚îÇ   ‚îú‚îÄ‚îÄ test.record
	‚îÇ   ‚îî‚îÄ‚îÄ label_map.txt
	‚îî‚îÄ‚îÄ models
	    ‚îî‚îÄ‚îÄ research
	        ‚îî‚îÄ‚îÄ object_detection

Quelques commandes shell suffisent pour cr√©er les premiers niveaux de cette arborescence :

```bash	
# From within tod_tf2
(tf2) jlc@pikatchou $ mkdir -p images/faces_cubes/test
(tf2) jlc@pikatchou $ mkdir -p images/faces_cubes/train
(tf2) jlc@pikatchou $ mkdir pre_trained
(tf2) jlc@pikatchou $ mkdir -p training/faces_cubes
```
	 
## 4. T√©l√©charger le r√©seau pr√©-entra√Æn√©

Deux grandes familles de r√©seaux d√©di√©es √† la d√©tection d‚Äôobjets dans des images sont propos√©s sur Le d√©p√¥t git [TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)¬†:

* Les r√©seaux __R-CNN__ (_Region-based Convolutional Neural Network_) : bas√©s sur le concept de __recherche cibl√©e__ (_selective search_). Au lieu d‚Äôappliquer une seule fen√™tre √† toutes les positions possibles de l‚Äôimage, l‚Äôalgorithme de recherche cibl√©e g√©n√®re 2000 propositions de r√©gions d‚Äôint√©r√™ts o√π il est le plus probable de trouver des objets √† d√©tecter. Cet algorithme se base sur des √©l√©ments tels que la texture, l‚Äôintensit√© et la couleur des objets qu‚Äôil a appris √† d√©tecter pour proposer des r√©gions d‚Äôint√©r√™t. Une fois les 2000 r√©gions choisies, la derni√®re partie du r√©seau produit la probabilit√© que l‚Äôobjet dans la r√©gion appartienne √† chaque classe.
Il existe √©galement des versions __Fast R-CNN__ et __Faster R-CNN__ qui permettent de rendre l‚Äôentra√Ænement plus rapide;

* Les r√©seaux __SSD__ (_Single Shot Detector_) : font partie des d√©tecteurs consid√©rant la d√©tection d‚Äôobjets comme un probl√®me de r√©gression les plus connus. L'algorithme __SSD__ utilise d‚Äôabord un r√©seau de neurones convolutif pour produire une carte des points cl√©s dans l‚Äôimage puis, comme __Faster R-CNN__, utilise des cadres de diff√©rentes tailles pour traiter les √©chelles et les ratios d‚Äôaspect.

La diff√©rence entre Faster R-CNN et SSD est qu‚Äôavec R-CNN on r√©alise une classification sur chacune des 2000 fen√™tres g√©n√©r√©es par l‚Äôalgorithme de recherche cibl√©e, alors qu‚Äôavec SSD on cherche √† pr√©dire la classe ET la fen√™tre de l‚Äôobjet, en m√™me temps. SSD apprend les d√©calages √† appliquer sur les cadres utilis√©es pour encadrer au mieux l‚Äôobjet plut√¥t que d‚Äôapprendre les fen√™tres en elle-m√™mes (ce que fait Faster R-CNN). Cela rend SSD plus rapide que Faster R-CNN, mais √©galement moins pr√©cis.

üì• Pour le travail de reconnaissance des faces des cubes dans les images fournies par la cam√©ra du robot Ergo Jr tu peux t√©l√©charger le r√©seau `SSD MobileNet V1 FPN 640x640` sur le site [TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md).

Une fois t√©l√©charg√©, il faut extraire l'archive TGZ au bon endroit de l'arborescence de travail :
```bash
# From within tod_tf2
(tf2) jlc@pikatchou $ tar xvzf ~/T√©l√©chargements/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8.tar.gz -C pre_trained
```
puis cr√©er le dossier `ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8` dans le dossier `training/faces_cubes` :
```bash	
(tf2) jlc@pikatchou $ mkdir training/faces_cubes/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8
```
On v√©rifie :

	# From within tod_tf2
	(tf2) jlc@pikatchou $ tree -d pre_trained
	pre_trained
	‚îî‚îÄ‚îÄ ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8
	    ‚îú‚îÄ‚îÄ checkpoint
	    ‚îî‚îÄ‚îÄ saved_model
	        ‚îî‚îÄ‚îÄ variables
	        
	(tf2) jlc@pikatchou $ tree -d training
	training
	‚îî‚îÄ‚îÄ faces_cubes
	    ‚îî‚îÄ‚îÄ ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8


## 4. Cr√©ation des donn√©es pour l'apprentissage supervis√©

Ce travail se d√©compose en plusieurs √©tapes

1. Cr√©ation des images avec la cam√©ra du robot -> fichiers \*.jpg, \*.png
2. Annotation des images avec le logiciel labelImg -> fichiers \*.xml
3. Conversion des fichiers annot√©s \*.xml au format CSV
4. Conversion des fichiers annot√©s CSV au format _tensorflow record_
5. Cr√©er du fichier `label_map.pbtxt` qui contient les labels des objets √† reconna√Ætre.


### 4.1 Cr√©ation des images avec la cam√©ra du robot  

Les images des faces des cubes peuvent √™tre faites en utilisant le service ROS `/get_image` propos√© par le robot Poppy Ergo Jr.

image001.png               |  image002.png
:-------------------------:|:-------------------------:
![image1](img/image.png)   |  ![image2](img/image001.png)


ü§ñ Rappels : lancement du ROS Master et des services ROS sur le robot :
 
* allumer le robot Poppy Ergo Jr,
* se connecter sur la carte RPi du robot : `ssh pi@poppy.local` (mdp: `raspberry`) 
* ‚úÖ v√©rifier que `ROS_MASTER_URI` pointe bien vers `poppy.local:11311` :
```
pi@poppy:~ $ env|grep ROS_MASTER
ROS_MASTER_URI=http://poppy.local:11311
```	
* si `ROS_MASTER_URI` n'est pas bon, √©dite le fichier `~/.bahrc` du robot et mets la bonne valeur...
* Lance le ROS Master et les services ROS sur le robot avec la commande : `roslaunch poppy_controllers control.launch`

üíª Et maintenant dans un terminal sur ton PC :
* ‚úÖ v√©rifie que `ROS_MASTER_URI` pointe bien vers `poppy.local:11311` :
```bash
(tf2) jlc@pikatchou:~ $ env|grep ROS_MASTER
ROS_MASTER_URI=http://poppy.local:11311
```	
* si `ROS_MASTER_URI` n'est pas bon, √©dite ton fchier `~/.bahrc` entmets la bonne valeur...


üêç Tu peux maintenant utiliser le programme Python `get_image_from_ergo.py` pour cr√©er des images nomm√©es `imagesxxx.png` (`xxx` = `001`, `002`...) avec l'appui sur la touche Enter pour passer d'une prise d'image √† l'autre :
```python
import cv2, rospy
from poppy_controllers.srv import GetImage
from cv_bridge import CvBridge

i=1
while True:
    get_image = rospy.ServiceProxy("get_image", GetImage)
    response = get_image()
    bridge = CvBridge()
    image = bridge.imgmsg_to_cv2(response.image)
    cv2.imwrite(f"image{i:03d}.png", image)
    cv2.imshow("Poppy camera", image)
    rep = input("Enter...")
    #cv2.waitKey(0)
    i += 1
```

si tu obtiens l'erreur : `ModuleNotFoundError: No module named 'rospkg'`, il faut simplement ajouter le module Python `rospkg` √† ton EVP `tf2` :
```bash
(tf2) jlc@pikatchou:~ $ pip install rospkg
```


Chaque √©quipe peut faire quelques dizaines d'images variant les faces des cubes visibles puis les images peuvent √™tre d√©pos√©es sur un serveur pour servir √† toutes les √©quipes.

Une fois collect√©es toutes les images, il faut mettre environ 90 % des images dans le dossier `images\faces_cubes\train` et le reste des images dans le dossier `images\faces_cubes\test`.

### 4.2 Annoter des images avec le logiciel labelImg

L'annotation des images peut √™tre faite de fa√ßon tr√®s simple avec le logiciel `labelImg`.
C‚Äôest une √©tape du travail qui prend du temps¬†et qui peut √™tre r√©alis√©e √† plusieurs en se r√©partissant les images.

L'installation du module Python `labelImg` faite dans l'EVP `tf2` (cf section 2.) permet de lancer le logiciel `labelImg` en tapant¬†:
```bash
(tf2) jlc@pikatchou:~ $ labelImg
```

Utilise les boutons [Open Dir] et [Change Save Dir] pour te positionner dans le dossier `images/face_cubes/train/`.<br>
La premi√®re image est automatiquement charg√©e dans l'interface graphique :

![labelImg_2.png](img/labelImg_2.png)

Pour chaque image, tu dois annoter les objets √† reconna√Ætre :
* avec le bouton [Create RectBox], tu entoures une face de cube,
* la boite des label s'ouvre alors et tu dois √©crire le blabel `one` ou `two` en fonction de la face s√©lectionn√©e,
* it√®re le processus pour chacune des faces de cubes pr√©sente dans l'image...

    premi√®re face          |  deuxi√®me face            |  fin
:-------------------------:|:-------------------------:|:-------------------------:
![1](img/labelImg_3.png)   |  ![2](img/labelImg_4.png) | ![3](img/labelImg_5.png)

* quand c'est fini, tu cliques sur le bouton [Save] et tu passes √† l'image suivante avec le bouton [Next Image].
* Une fois toutes les images annot√©es, utilise les boutons [Open Dir] et [Change Save Dir] pour travailler avec les images du dossier `images/face_cubes/test/` et annote toutes les images de test.

### 4.3 Convertir les fichiers XML annot√©s au format CSV

Cette √©tape permet de synth√©tiser dans un fichier CSV unique les donn√©es d‚Äôapprentissage contenues dans les diff√©rents fichiers XML cr√©es √† l‚Äô√©tape d'annotation. 
Le programme `tod_tf2/xml_to_csv_tt.py` permet de g√©n√©rer les deux fichiers CSV correspondant aux donn√©es d‚Äôapprentissage et de test. 
Depuis le dossier `tod_tf2` tape la commande suivante¬†:

```bash
# From within tod_tf2
(tf2) jlc@pikatchou:~ $ python xml_to_csv_tt.py -p faces_cubes
Successfully converted xml data in file <images/faces_cubes/train_labels.csv>.
Successfully converted xml data in file <images/faces_cubes/test_labels.csv>.
```
Les fichiers `train_labels.csv` et `test_labels.csv` sont cr√©√©s dans le dossier  `images/faces_cubes/` :

### 4.4 Convertir des fichiers CSV annot√©s au format _tfrecord_

Pour cette √©tape, on utilise le programme `tod_tf2/generate_tfrecord_tt.py`. Depuis le dossier `tod_tf2` tape la commande :
```bash
# From within tod_tf2
(tf2) jlc@pikatchou:~ $ python generate_tfrecord_tt.py --project faces_cubes
Successfully created the TFRecord file: ./training/faces_cubes/train.record
Successfully created the TFRecord file: ./training/faces_cubes/test.record
```
Avec cette commande on vient de cr√©er les 2 fichiers `train.record` et `test.record` dans le dossier `training/faces_cubes`¬†: ce sont ces deux fichiers qui serviront pour l‚Äôentra√Ænement du r√©seau.

### 4.5 Cr√©er le fichier label_map.pbtxt
 
La derni√®re √©tape consiste a cr√©er le fichier `label_map.pbtxt` dans le dossier `training/faces_cubes`. Ce fichier d√©crit la ¬´¬†carte des labels¬†¬ª (label map) n√©cessaire √† l‚Äôentra√Ænement du r√©seau. 
La carte des labels permet de conna√Ætre l‚ÄôID (nombre entier) associ√© √† chaque √©tiquette (_label_) identifiant les objets √† reconna√Ætre. La structure type du fichier est la suivante¬†:


	 item {
	   id: 1
	   name: ‚Äòobjet_1‚Äô
	 }
	 item {
	   id: 2
	   name: ‚Äòobjet_2‚Äô
	 }
	 ...

Pour le projet `face_cubes`, le contenu du fichier `training/faces_cubes/label_map.pbtxt` √† cr√©er est :

	 item {
	   id: 1
	   name: ‚Äòone‚Äô
	 }
	 item {
	   id: 2
	   name: ‚Äòtwo‚Äô
	 }

## 5. Entra√Ænement supervis√© du r√©seau pr√©-entra√Æn√©

Ce travail se d√©compose en deux √©tapes

1. Cr√©ation du fichier de configuration de l'entra√Ænement.
2. Lancement de l'entra√Ænement supervis√©.


### 5.1 Cr√©ation du fichier de configuration de l'entra√Ænement.

C‚Äôest la derni√®re √©tape avant de pouvoir lancer l‚Äôentra√Ænement‚Ä¶

Le fichier de configuration `pipeline.config` pr√©sent dans le dossier du r√©seau pr√©-entra√Æn√© `pre_trained/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8` doit √™tre copi√© dans le dossier `training/faces_cubes\ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8`. 

Il faut ensuite modifier les param√®tres pour l‚Äôentra√Ænement et le test en fonction : 
* ligne 3 -- remplacer `num_classes: 90` par `num_classes: 2`, puisqu'on n'a que deux classes pour le projet `faces_cubes`
* ligne 131 -- remplacer `batch_size: 64` par `batch_size: 4` : c'est le nombre d'image trait√©e avant de mettre √† jour les poids du r√©seau de neurones.<br>
Si cette valeur est trop grande, le calcul risque de d√©passer la capacit√© m√©moire RAM de ta machine... √† r√©gler en focntion de la quantit√© de RAM de ta machine.
* ligne 161 -- remplacer `fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED"` par `fine_tune_checkpoint: "pre_trained/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0", c'est le chemin du dossier pr√©-entrain√© qui contient les poids du r√©seau apr√®s le pr√©-entra√Ænement.
* ligne 162 -- remplacer `num_steps: 25000` par `num_steps: 1000`; c'est le nombre max d'it√©rations de l'entra√Ænement, pour des machines avec un CPU peu puissant on n'ira peut √™tre m√™me pas jusqu'√† 1000, ce serait trop long...
* ligne 165 -- remplacer `max_number_of_boxes: 100` par `max_number_of_boxes: 5`; on n'a pas plus de 5 faces de cubes sur une images, √ßa √©conomise de la RAM.
* ligne 167 -- remplacer `fine_tune_checkpoint_type: "classification"` par `fine_tune_checkpoint_type: "detection"` pour activer l'algorithme de d√©tection.
* ligne 172 : remplacer `input_path: "PATH_TO_BE_CONFIGURED"` par `input_path: "./training/faces_cubes/"`, c'est le chemin du dossier contenant le fichier `train.record`.
* ligne 181 -- remplacer `label_map_path: "PATH_TO_BE_CONFIGURED"` par `label_map_path: "./training/faces_cubes/"`, c'est le chemin du dossier contenant le fichier `label_map.pbtxt`.
* ligne 185 -- remplacer `input_path: "PATH_TO_BE_CONFIGURED"` par `input_path: "./training/faces_cubes/"`, c'est le chemin du dossier contenant le fichier `test.record`.

## 5.2 Lancer l'entra√Ænement

* copier le fichier `models\research\object_detection\model_main_tf2.py` dans la racine `tod_tf2`,
* se placer √† la racine du projet dans le dossier `tod_tf2`,
* taper la commande :
```bash
(tf2) jlc@pikatchou:~ $ python model_main_tf2.py --model_dir=training/faces_cubes/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/  \
                                                 --pipeline_config_path=training/faces_cubes/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/pipeline.config 
```
le programme Python lanc√© est tr√®s verbeux... <br>
au bout d'un temps qui peut √™tre assez long (plusieurs minutes avec un CPU ), les logs de l'entra√Æenement apparaissent √† l'√©cran :



En cas d'arr√™t brutal du programme avec le message "Processus arr√™t√©", ne pas h√©siter √† diminer la valeur du param√®tre `batch_size` jusqu√† 2 si n√©cessaire.... <br>
M√™me avec un `batch_size` de 2, le processus Python peut n√©cessiter jusqu'√† 2 ou 3 Go de RAM pour lui tout seul, ce qui peut mettre certains portables en difficult√©...




## 6. √âvaluation des inf√©rences du r√©seau

Une fois le r√©deau entra√Æn√© avec les donn√©es d'entra√Ænement et de test, on peut √©valuer qualitativement la qualit√© du r√©seau entra√Æn√© en utilisant de nouvelles images.

L'id√©e ici est de cr√©er de nouvelles images et de v√©rifier que le r√©seau entra√Æn√© est bien capables de d√©tecter les faces des cubes en discriminant correctement les num√©ros √©crits sur les faces des cubes.


## 7. Int√©gration

Il est maintenant temps d'int√©grer les deux parties du pipeline pour l'utilisation finale. Ouvrez le fichier `main.py` √† la racine du projet.

Ex√©cuter maintenant le programme `main.py` : donner le chemin d'un dossier qui contient les fichiers du r√©seau entra√Æn√© et vous devriez commencer √† obtenir la reconnaissance des chiffres '1' et '2' dans les images fournies.

Il faudra certainement refaire plusieurs fois l'entra√Ænement du r√©seau en jouant sur plusieurs param√®tres avant d'obtenir un r√©seau entra√Æn√© qui fonctionne correctement :

* augmenter/diminuer `BATCH_SIZE` peut modifier les temps de calcul et la qualit√© du r√©seau entra√Æn√©...

Pour confirmer la qualit√© de votre r√©seau entra√Æn√© vous pouvez enregistrer vos propres fichiers PNG avec les images faites avec la cam√©ra du robot en utilisant le service ROS `/get_image`. 

Lancer le programme et observer les performances de votre r√©seau op√©rant sur vos propres images.

